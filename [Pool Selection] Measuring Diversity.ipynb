{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e9878b",
   "metadata": {},
   "source": [
    "## Diversity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a23a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "import math \n",
    "from os import path\n",
    "import pandas as pd \n",
    "from tqdm import tqdm  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functions import *  \n",
    "\n",
    "### preprocessing libraries \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "### model libraries \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "### static \n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier \n",
    "\n",
    "\n",
    "### libraries for metrics \n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             precision_score,\n",
    "                             recall_score, \n",
    "                             f1_score,\n",
    "                             roc_auc_score, \n",
    "                             precision_recall_curve,\n",
    "                             balanced_accuracy_score,\n",
    "                             auc) \n",
    "\n",
    "### data balancing libraries \n",
    "from imblearn.over_sampling import SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acee5d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def side_by_side(*objs, **kwds):\n",
    "    from pandas.io.formats.printing import adjoin\n",
    "    space = kwds.get('space', 4)\n",
    "    reprs = [repr(obj).split('\\n') for obj in objs]\n",
    "    print (adjoin(space, *reprs))\n",
    "    print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261fb1c6",
   "metadata": {},
   "source": [
    "#### Load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed780b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_filename       = \"data/assessment_statistics.csv\"\n",
    "cogniteive_score_filename = \"data/cogniteive_score_statistics.csv\"\n",
    "mri_filename              = \"data/mri_statistics.csv\"   \n",
    "four_labels_filename      = \"data/four_labels.csv\" \n",
    "\n",
    "baselineDF = pd.read_csv(\"data/Baseline_final.csv\") \n",
    "assessmentDF      = pd.read_csv(assessment_filename)\n",
    "cognitive_scoreDF = pd.read_csv(cogniteive_score_filename)\n",
    "mriDF             = pd.read_csv(mri_filename) \n",
    "four_labelsDF     = pd.read_csv(four_labels_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c9405a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1371, 146)\n"
     ]
    }
   ],
   "source": [
    "complete_datasetDF = assessmentDF[:]\n",
    "# complete_datasetDF = pd.merge(assessmentDF, cognitive_scoreDF, on=\"RID\", how=\"inner\") \n",
    "complete_datasetDF = pd.merge(complete_datasetDF, mriDF, on=\"RID\", how=\"inner\") \n",
    "complete_datasetDF = pd.merge(complete_datasetDF, baselineDF, on=\"RID\", how=\"inner\") \n",
    "\n",
    "# complete_datasetDF = pd.merge(complete_datasetDF, four_labelsDF, on=\"RID\", how=\"inner\") \n",
    "print(\"Shape: {}\".format(complete_datasetDF.shape)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4d420f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\"AD\": 0, \"sMCI\": 1, \"CN\": 2, \"pMCI\": 3} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333fc11b",
   "metadata": {},
   "source": [
    "### Single Best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7039ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_predictions(y, y_pred1, y_pred2):\n",
    "    \"\"\"Pre-process the predictions of a pair of base classifiers for the\n",
    "    computation of the diversity measures\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array of shape (n_samples):\n",
    "        class labels of each sample.\n",
    "    y_pred1 : array of shape (n_samples):\n",
    "              predicted class labels by the classifier 1 for each sample.\n",
    "    y_pred2 : array of shape (n_samples):\n",
    "              predicted class labels by the classifier 2 for each sample.\n",
    "    Returns\n",
    "    -------\n",
    "    N00 : Percentage of samples that both classifiers predict the wrong label\n",
    "    N10 : Percentage of samples that only classifier 2 predicts the wrong label\n",
    "    N10 : Percentage of samples that only classifier 1 predicts the wrong label\n",
    "    N11 : Percentage of samples that both classifiers predict the correct label\n",
    "    \"\"\"\n",
    "    size_y = len(y)\n",
    "    if size_y != len(y_pred1) or size_y != len(y_pred2):\n",
    "        raise ValueError(\n",
    "            'The vector with class labels must have the same size.')\n",
    "\n",
    "    N00, N10, N01, N11 = 0.00001, 0.00001, 0.00001, 0.00001\n",
    "    for index in range(size_y):\n",
    "        if y_pred1[index] == y[index] and y_pred2[index] == y[index]:\n",
    "            N11 += 1.0\n",
    "        elif y_pred1[index] == y[index] and y_pred2[index] != y[index]:\n",
    "            N10 += 1.0\n",
    "        elif y_pred1[index] != y[index] and y_pred2[index] == y[index]:\n",
    "            N01 += 1.0\n",
    "        else:\n",
    "            N00 += 1.0\n",
    "\n",
    "    return N00 / size_y, N10 / size_y, N01 / size_y, N11 / size_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0a15b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def Q_statistic(y, y_pred1, y_pred2):\n",
    "    \"\"\"Calculates the Q-statistics diversity measure between a pair of\n",
    "    classifiers. The Q value is in a range [-1, 1]. Classifiers that tend to\n",
    "    classify the same object correctly will have positive values of Q, and\n",
    "    Q = 0 for two independent classifiers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array of shape (n_samples):\n",
    "        class labels of each sample.\n",
    "    y_pred1 : array of shape (n_samples):\n",
    "              predicted class labels by the classifier 1 for each sample.\n",
    "    y_pred2 : array of shape (n_samples):\n",
    "              predicted class labels by the classifier 2 for each sample.\n",
    "    Returns\n",
    "    -------\n",
    "    Q : The q-statistic measure between two classifiers\n",
    "    \"\"\"\n",
    "    N00, N10, N01, N11 = _process_predictions(y, y_pred1, y_pred2)\n",
    "    Q = ((N11 * N00) - (N01 * N10)) / ((N11 * N00) + (N01 * N10))\n",
    "    return Q\n",
    "\n",
    "\n",
    "def correlation_coefficient(y, y_pred1, y_pred2):\n",
    "    \"\"\"Calculates the correlation  between two classifiers using oracle\n",
    "    outputs. Coefficient is a value in a range [-1, 1].\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array of shape (n_samples):\n",
    "        class labels of each sample.\n",
    "    y_pred1 : array of shape (n_samples):\n",
    "              predicted class labels by the classifier 1 for each sample.\n",
    "    y_pred2 : array of shape (n_samples):\n",
    "              predicted class labels by the classifier 2 for each sample.\n",
    "    Returns\n",
    "    -------\n",
    "    rho : The correlation coefficient measured between two classifiers\n",
    "    \"\"\"\n",
    "    N00, N10, N01, N11 = _process_predictions(y, y_pred1, y_pred2)\n",
    "    tmp = (N11 * N00) - (N10 * N01)\n",
    "    rho = tmp / np.sqrt((N11 + N01) * (N10 + N00) * (N11 + N10) * (N01 + N00))\n",
    "    return rho\n",
    "\n",
    "\n",
    "def disagreement_measure(y, y_pred1, y_pred2):\n",
    "    \"\"\"Calculates the disagreement measure between a pair of classifiers. This\n",
    "    measure is calculated by the frequency that only one classifier makes the\n",
    "    correct prediction.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array of shape (n_samples):\n",
    "        class labels of each sample.\n",
    "    y_pred1 : array of shape (n_samples):\n",
    "              predicted class labels by the classifier 1 for each sample.\n",
    "    y_pred2 : array of shape (n_samples):\n",
    "              predicted class labels by the classifier 2 for each sample.\n",
    "    Returns\n",
    "    -------\n",
    "    disagreement : The frequency at which both classifiers disagrees\n",
    "    \"\"\"\n",
    "    _, N10, N01, _ = _process_predictions(y, y_pred1, y_pred2)\n",
    "    disagreement = N10 + N01\n",
    "    return disagreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b47a1349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_diversity(model1, model2, dataset, target_column, test_size, seed, scaler_type): \n",
    "    X_train, X_test, y_train, y_test = split_dataset(dataset, target_column, test_size, seed) \n",
    "    \n",
    "    scaled_X_train, scaled_X_test = normalize_dataset(X_train, X_test, scaler_type) \n",
    "    \n",
    "    ## 4. balancing data \n",
    "    balanced_trainX, balanced_trainY, balanced_testX, balanced_testY = balance_data(scaled_X_train, y_train, \n",
    "                                                                                        scaled_X_test, y_test) \n",
    "    \n",
    "    model1.fit(balanced_trainX, balanced_trainY)\n",
    "    model2.fit(balanced_testX, balanced_testY) \n",
    "    \n",
    "    y_pred1 = model1.predict(balanced_testX)\n",
    "    y_pred2 = model2.predict(balanced_testX)\n",
    "    \n",
    "    q_statistic       = Q_statistic(balanced_testY.values, y_pred1, y_pred2) \n",
    "    corr_score        = correlation_coefficient(balanced_testY, y_pred1, y_pred2) \n",
    "    disagreement      = disagreement_measure(balanced_testY, y_pred1, y_pred2) \n",
    "    \n",
    "    return q_statistic, corr_score, disagreement \n",
    "\n",
    "\n",
    "def get_diversity(model, compared_models, dataset, target_column, test_size, seed, scaler_type): \n",
    "    \n",
    "    q_statistic_list  = []\n",
    "    corr_score_list   = [] \n",
    "    disagreement_list = [] \n",
    "    names = [] \n",
    "    \n",
    "    for c_model in compared_models:  \n",
    "        q, c, d = measure_diversity(model, c_model, dataset, target_column, test_size, seed, scaler_type) \n",
    "        q_statistic_list.append(q)\n",
    "        corr_score_list.append(c)\n",
    "        disagreement_list.append(d)\n",
    "        \n",
    "        names.append(c_model.__class__.__name__)\n",
    "    \n",
    "    result = {\"model\": names, \n",
    "              \"q_statistic\": q_statistic_list, \n",
    "              \"corr_score\": corr_score_list, \n",
    "              \"disagreement\": disagreement_list}\n",
    "    \n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a2df0bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN = \"DX\" \n",
    "TEST_SIZE     = 0.20 \n",
    "SPLIT_SEEDS   = [45, 78, 95, 15, 53, 12, 85, 61, 77, 10] \n",
    "SCALER_TYPE   = \"mm\" # ss for Sdandard Scaler, mm for MinMax Scaler \n",
    "CORR_LIMIT    = 0.70 \n",
    "CV_K          = 5 \n",
    "\n",
    "dataset       = complete_datasetDF.drop(['RID'], axis=1) \n",
    "\n",
    "mapping = {\"AD\": 0, \"sMCI\": 1, \"CN\": 2, \"pMCI\": 3}\n",
    "\n",
    "dataset['DX'] = dataset['DX'].map(mapping)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "e75957aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_list = [KNeighborsClassifier(), \n",
    "              RandomForestClassifier(random_state=42),  \n",
    "              GaussianNB(), \n",
    "              MLPClassifier(solver='adam', random_state=42),  \n",
    "              SVC(probability=True, random_state=42),   \n",
    "              LGBMClassifier(random_state=42), \n",
    "              LogisticRegression(), \n",
    "              XGBClassifier(random_state=42), \n",
    "             ] \n",
    "\n",
    "diversityDF = pd.DataFrame() \n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "SPLIT_SEEDS = [45, 78, 95, 15, 53, 12, 85, 61, 77, 10]\n",
    "\n",
    "for seed in SPLIT_SEEDS: \n",
    "    res = get_diversity(model, model_list, dataset, TARGET_COLUMN, TEST_SIZE, seed, SCALER_TYPE) \n",
    "    diversityDF = pd.concat([diversityDF, res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "74cdc468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_formatter(df): \n",
    "    columns = df.columns[1:]\n",
    "    result = {}\n",
    "    \n",
    "    for col in columns: \n",
    "        result[col] = \"{}±{}\".format(round(df[col].mean(), 3), round(df[col].std(), 3))  \n",
    "    \n",
    "    return pd.DataFrame(result, index = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "8f70749c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_statistic</th>\n",
       "      <th>corr_score</th>\n",
       "      <th>disagreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.554±0.146</td>\n",
       "      <td>0.136±0.12</td>\n",
       "      <td>0.269±0.021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   q_statistic  corr_score disagreement\n",
       "0  0.554±0.146  0.136±0.12  0.269±0.021"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_formatter(diversityDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4844180a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
